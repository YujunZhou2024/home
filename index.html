<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Hi, Jon Here. Please DELETE the two <script> tags below if you use this HTML, otherwise my analytics will track your page -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=UA-7580334-2"></script> -->
  <!-- <script type="text/javascript" src="js/hidebib.js"></script> -->
  <!-- <script src="pet_cursor.js"></script>  -->
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-7580334-2');
  </script>

  <title>Cihang Xie</title>
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">
  
  <meta name="author" content="Cihang Xie">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="stylesheet" type="text/css" href="css/style.css">
  <link rel="icon" type="image/png" href="images/UCSC_icon.png">
</head>

<!-- <body onload="startPetCursor();"> -->
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:70%;vertical-align:middle">
              <p style="text-align:center">
                <name>Cihang Xie</name>
              </p>
              <p>I am an Assistant Professor of <a href="https://www.soe.ucsc.edu/departments/computer-science-and-engineering">Computer Science and Engineering</a> at <a href="https://www.ucsc.edu/">University of California, Santa Cruz</a>. My research interest lies at the intersection of computer vision and machine learning, with the goal of building human-level computer vision systems. I am particularly interested in securing model performance under distribution shifts, and developing deep representation learning with minimal supervision.
              </p>


              <p>I received my Ph.D. degree from <a href="https://www.jhu.edu/">Johns Hopkins University</a>, advised by <a href="http://www.cs.jhu.edu/~ayuille/">Bloomberg Distinguished Professor Alan Yuille</a>. I have worked as a research intern with <a href="http://kaiminghe.com/">Kaiming He</a> and <a href="https://lvdmaaten.github.io/">Laurens van der Maaten</a> at the Facebook AI Research (FAIR); <a href="https://cs.stanford.edu/~quocle/">Quoc Le</a> at the Google Brain. I receive the <a href="https://research.fb.com/fellows/xie-cihang/">2020 Facebook Fellowship</a>.
              </p>
<!--               <p style="text-align:center">
                <a href="mailto:cixie@ucsc.edu">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=X3vVZPcAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/cihangxie">Github</a>
              </p> -->
              <p style="text-align:center">
                </br>
                <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
                <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
                <a href="mailto:cixie@ucsc.edu"><i class="fa fa-envelope" style='font-size:30px'></i>&nbsp &nbsp
                <a href="https://scholar.google.com/citations?user=X3vVZPcAAAAJ&hl=en"><i class="ai ai-google-scholar ai-3x" style='font-size:28px'></i>&nbsp &nbsp
                <a href="https://github.com/cihangxie"><i class="fa fa-github" style='font-size:30px'></i>&nbsp &nbsp
                <a href="https://twitter.com/cihangxie?ref_src=twsrc%5Etfw" class="twitter-follow-button" data-show-count="false">Follow @cihangxie</a><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/CihangXie.jpg"><img style="width:100%;max-width:100%;border-radius:50%" alt="profile photo" src="images/CihangXie.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                    <p><font color="magenta"><strong>Prospective graduate students</strong></font>: My group has <strong>multiple fully-funded Ph.D. positions for Fall 2023</strong>. If you are interested in working with me, please submit your application <a href="https://grad.soe.ucsc.edu/admissions">here</a> and mention me as a potential advisor. You are welcomed to drop me an email if you think our research interests are strongly aligned.
                     <!-- <font color="red"><strong>Application deadline has passed.</strong> -->
                     </font></p>
                    
                    <p><font color="orange"><strong>Prospective summer interns & visiting students</strong></font>: My group has multiple positions for summer interns and visiting students. If you are interested, please send me an email with your CV, transcript, and publications (if any).</p>

                    <p><font color="red"><strong>Due to the large amount of emails I receive, I may not be able to respond to each one individually.</strong></font>

                    <p><font color="olive"><strong>Seminar</strong></font>: I am co-organizing a weekly seminar on <a href="https://vsehwag.github.io/SPML_seminar/">Security & Privacy in Machine Learning</a>. If you are interested, please <a href="https://groups.google.com/forum/#!forum/spml-seminars/join">join our mailing list</a> or <a href="https://calendar.google.com/calendar/u/0?cid=N2FwbTVxYzJsOGM2bXBiNGY4am1oMjNsdGNAZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ"> subscribe to our event calendar</a>.</p>
                </td>
            </tr>
        </tbody></table>
        <hr>

        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                    <heading>Recent News</heading>
                </td>
            </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <ul>
<!--              <li><strong>[<del><font color="red">Call for Papers</font></del>]</strong> I am co-organizing the Workshop on <a href="https://adv-workshop-2020.github.io/">Adversarial Machine Learning in Computer Vision</a> in CVPR 2020. <del>Please submit your paper <a href="https://cmt3.research.microsoft.com/CVPRamlcv2020">here</a>!</del></li>-->
              <!-- <li><strong>[<font color="red">Workshop@ICLR2021</font>]</strong> I am co-organizing the Workshop on <a href="https://aisecure-workshop.github.io/aml-iclr2021/">Security and Safety in Machine Learning Systems</a> in ICLR 2021.</li>
              <li><strong>[<font color="red">Workshop@CVPR2021</font>]</strong> I am co-organizing the Workshop on <a href="https://cvpr21-nas.com/">Neural Architecture Search: 1st Lightweight NAS Challenge and Moving Beyond</a> in CVPR 2021. Stay tuned for more details coming soon.</li>
              <li><strong>[<font color="red">Tutorial@CVPR2021</font>]</strong> I am co-organizing the Tutorial on <a href="https://advmlincv.github.io/cvpr21-tutorial/">Adversarial Machine Learning in Computer Vision</a> in CVPR 2021. Stay tuned for more details coming soon.</li> -->
              <li><strong>[<font color="red">October 2022</font>]</strong> I will be giving a talk in <a href="https://eccv22-arow.github.io/">ECCV 2022 Workshop on Adversarial Robustness in the Real World</a>.</li>

              <li><strong>[<font color="red">September 2022</font>]</strong> Two papers are accepted by NeurIPS 2022.</li>

              <li><strong>[<font color="red">July 2022</font>]</strong> Two papers are accepted by ECCV 2022.</li>

              <li><strong>[June 2022]</strong> I will be giving a talk in <a href="https://artofrobust.github.io/">CVPR 2022 Workshop on The Art of Robustness: Devil and Angel in Adversarial Machine Learning</a>.</li>

              <li><strong>[March 2022]</strong> Two papers are accepted by CVPR 2022.</li>

              <li><strong>[February 2022]</strong> I will be giving a talk in <a href="https://practical-dl.github.io/">AAAI 2022 1st International Workshop on Practical Deep Learning in the Wild</a>.</li>

              <li><strong>[January 2022]</strong> <a href="https://openreview.net/forum?id=hcoswsDHNAW">Fast AdvProp</a> and <a href="https://openreview.net/forum?id=ydopy-e6Dg">iBOT</a> are accepted by ICLR 2022.</li>

              <li><strong>[December 2021]</strong> I gave a talk in <a href="http://ai.bu.edu/visda-2021/">NeurIPS 2021 Visual Domain Adaptation Challenge</a>.</li>

              <li><strong>[October 2021]</strong> I gave a talk in <a href="https://iccv21-adv-workshop.github.io/">ICCV 2021 2nd Workshop on Adversarial Robustness In the Real World</a>.</li>

              <li><strong>[September 2021]</strong> Our work on <a href="https://arxiv.org/abs/2111.05464">Robustness Comparison between Transformers and CNNs</a> is accepted by NeurIPS 2021.</li>

              <li><strong>[August 2021]</strong> Our work on <a href="https://arxiv.org/abs/2110.00519">Towards Symbolic Reasoning on Real Images</a> is accepted by ICCV 2021.</li>

              <li><strong>[July 2021]</strong> I gave a talk in <a href="https://advml-workshop.github.io/icml2021/">ICML 2021 Workshop on A Blessing in Disguise: The Prospects and Perils of Adversarial Machine Learning</a>.</li>
              
              <li><strong>[June 2021]</strong> I gave a talk in <a href="https://advmlincv.github.io/cvpr21-tutorial/">CVPR 2021 Tutorial on Adversarial Machine Learning in Computer Vision</a>.</li>
             
              <!-- <li><strong>[June 2021]</strong> I am serving as an Area Chair for ICLR 2022.</li> -->
              
              <!-- <li><strong>[February 2021]</strong> Our work on <a href="https://arxiv.org/abs/2103.13886">Robust and Accurate Object Detection</a> is accepted by CVPR 2021.</li> -->
              
              <!-- <li><strong>[January 2021]</strong> Our work on <a href="https://arxiv.org/abs/2010.05981">Shape-Texture Debiased Learning</a> is accepted by ICLR 2021.</li> -->

              <li> I am serving as an Area Chair for ICLR 2023, NeurIPS 2022, ECCV 2022, ICLR 2022, ICCV 2021, and a Senior Program Committee for AAAI 2022, IJCAI 2021.</li>
              
              
              
              <!-- <li><strong>[October 2020]</strong> I am serving as a Senior Program Committee for IJCAI 2021.</li> -->
              
              <!-- <li><strong>[August 2020]</strong> I am co-organizing the Workshop on <a href="https://eccv20-adv-workshop.github.io/">Adversarial Robustness in the Real World</a> in ECCV 2020 <a href="https://www.youtube.com/watch?v=lDptIbsMIE0&list=PLWqw4ACzC-1XnyywVl53wc7lEOzSyhHYM">[video]</a>.</li> -->
              
              <!-- <li><strong>[July 2020]</strong> Two papers [<a href="https://arxiv.org/abs/1904.00979">1</a>, <a href="https://arxiv.org/abs/2004.05682">2</a>] are accepted by ECCV 2020.</li> -->
              
              <!-- <li><strong>[June 2020]</strong> I am co-organizing the Workshop on <a href="https://adv-workshop-2020.github.io/">Adversarial Machine Learning in Computer Vision</a> in CVPR 2020 <a href="https://www.youtube.com/watch?v=FCTf5MeIBFM&list=PLT6XqV0BKKrKN7GwULt9Z6hVxYd3PSaoy">[video]</a>.</li> -->
              <!-- <li><strong>[February 2020]</strong> Three papers [<a href="https://arxiv.org/abs/1911.09665">1</a>, <a href="https://arxiv.org/abs/2004.01961">2</a>, <a href="https://arxiv.org/abs/1909.04326">3</a>] are accepted by CVPR 2020.</li> -->
              <!-- <li><strong>[December 2019]</strong> <a href="https://openreview.net/forum?id=HyxJhCEFDS">One paper</a> is accepted by ICLR 2020.</li>
              <li><strong>[November 2019]</strong> <a href="https://cs.jhu.edu/~alanlab/Pubs20/li2020learning.pdf">One paper</a> is accepted by AAAI 2020.</li> -->
            </ul>
          </td>
        </tr>
        </tbody></table>
        <hr>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                    <heading>Events</heading>
                </td>
            </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <ul>
<!--              <li><strong>[<del><font color="red">Call for Papers</font></del>]</strong> I am co-organizing the Workshop on <a href="https://adv-workshop-2020.github.io/">Adversarial Machine Learning in Computer Vision</a> in CVPR 2020. <del>Please submit your paper <a href="https://cmt3.research.microsoft.com/CVPRamlcv2020">here</a>!</del></li>-->

			  <li><strong>[<font color="red">Workshop@ECCV2022</font>]</strong> I am co-organizing the 3rd Workshop on <a href="https://eccv22-arow.github.io/">Adversarial Robustness in the Real World</a> in ECCV 2022. This workshop will give <font color="orange">THREE Best Paper Awards ($10,000 each, $30,000 in total)</font>. Please submit your paper <a href="https://cmt3.research.microsoft.com/AROW2022/Submission/Index">here</a>!</li>

              <li><strong>[Workshop@CVPR2022]</strong> I am co-organizing the Workshop on <a href="https://artofrobust.github.io/">The Art of Robustness: Devil and Angel in Adversarial Machine Learning</a> in CVPR 2022.</li>

              <li><strong>[Workshop@ICLR2022]</strong> I am co-organizing the Workshop on <a href="https://iclrsrml.github.io/">Socially Responsible Machine Learning</a> in ICLR 2022.</li>

              <li><strong>[Journal@Frontiers]</strong> I served as the Guest Editor of <a href="https://www.frontiersin.org/research-topics/17616/trustworthy-machine-learning">Frontiers in Big Data: Trustworthy Machine Learning</a>.</li>

              <li><strong>[Workshop@ICCV2021]</strong> I co-organized the 2nd Workshop on <a href="https://iccv21-adv-workshop.github.io/">Adversarial Robustness in the Real World</a> in ICCV 2021.</li>
                <!-- Please submit your paper <a href="https://cmt3.research.microsoft.com/AROW2021/">here</a>.</li> -->

              <li><strong>[Workshop@ICCV2021]</strong> I co-organized the Workshop on <a href="https://neural-architecture-ppf.github.io/">Neural Architectures: Past, Present and Future</a> in ICCV 2021.</li>
               <!-- Please submit your paper <a href="https://cmt3.research.microsoft.com/NeurArch2021">here</a>.</li> -->

			   <li><strong>[Workshop@ICML2021]</strong> I co-organized the Workshop on <a href="https://icmlsrml2021.github.io/">Socially Responsible Machine Learning</a> in ICML 2021.</a></li>

              <li><strong>[Workshop@CVPR2021]</strong> I co-organized the Workshop on <a href="https://cvpr21-nas.com/">Neural Architecture Search: 1st Lightweight NAS Challenge and Moving Beyond</a> in CVPR 2021.</li>
              
              <li><strong>[Tutorial@CVPR2021]</strong> I co-organized the Tutorial on <a href="https://advmlincv.github.io/cvpr21-tutorial/">Adversarial Machine Learning in Computer Vision</a> in CVPR 2021. The video recording is <a href="https://www.youtube.com/watch?v=AOfq-0Vhprc&list=PLT6XqV0BKKrLEXxUqW7k88pldpdYV1ldF">here</a>.</li>

              <li><strong>[Workshop@ICLR2021]</strong> I co-organized the Workshop on <a href="https://aisecure-workshop.github.io/aml-iclr2021/">Security and Safety in Machine Learning Systems</a> in ICLR 2021.</li>
              
              <li><strong>[Workshop@ECCV2020]</strong> I co-organized the Workshop on <a href="https://eccv20-adv-workshop.github.io/">Adversarial Robustness in the Real World</a> in ECCV 2020. The video recording is <a href="https://www.youtube.com/watch?v=lDptIbsMIE0&list=PLWqw4ACzC-1XnyywVl53wc7lEOzSyhHYM">here</a>.</li>
              <!-- <a href="https://www.youtube.com/watch?v=lDptIbsMIE0&list=PLWqw4ACzC-1XnyywVl53wc7lEOzSyhHYM">[video]</a> -->

              <li><strong>[Workshop@CVPR2020]</strong> I co-organized the Workshop on <a href="https://adv-workshop-2020.github.io/">Adversarial Machine Learning in Computer Vision</a> in CVPR 2020. The video recording is <a href="https://www.youtube.com/watch?v=FCTf5MeIBFM&list=PLT6XqV0BKKrKN7GwULt9Z6hVxYd3PSaoy">here</a>.</li>
              <!-- <a href="https://www.youtube.com/watch?v=FCTf5MeIBFM&list=PLT6XqV0BKKrKN7GwULt9Z6hVxYd3PSaoy">[video]</a> -->
            </ul>
          </td>
        </tr>
        </tbody></table>
        <hr>


<!--         <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                    <heading>Fundings</heading>
                </td>
            </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <ul>
              <li>I received <a href="https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-santa-cruz-xie-adversarial-robustness">a $265,000 gift from Open Philanthropy and Good Ventures Foundation</a> for supporting the adversarial machine learning research in my lab.</li>
            </ul>
          </td>
        </tr>
        </tbody></table>
        <hr> -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                    <heading>Current Students</heading>
                </td>
            </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <ul>
			  <li><a href="https://xhl-video.github.io/xianhangli/">Xianhang Li</a> (Ph.D. of 2021, UC Santa Cruz Chancellor's Fellowship Recipient)</li>
			  <li><a href="https://zichaoli.github.io/">Zichao Li</a> (Ph.D. of 2021)</li>
			  <li><a href="https://ljb121002.github.io/">Junbo Li</a> (Ph.D. of 2021)</li>
			  <li><a href="https://zeyuwang.netlify.app/">Zeyu Wang</a> (Ph.D. of 2021)</li>
			  <li><a href="https://scholar.google.com/citations?user=hLHzhqkAAAAJ">Li Liu</a> (Ph.D. of 2022, co-advised with <a href="https://yuyinzhou.github.io/">Yuyin Zhou</a>)</li>
              <li><a href="https://scholar.google.com/citations?user=nHKExN0AAAAJ">Jieru Mei</a> (affiliated Ph.D. student from Johns Hopkins University)</li>
              <li><a href="https://scholar.google.com/citations?user=lU3wroMAAAAJ">Yixiao Zhang</a> (affiliated Ph.D. student from Johns Hopkins University)</li>
              <li><a href="https://scholar.google.com/citations?user=N1-l4GsAAAAJ">Yutong Bai</a> (affiliated Ph.D. student from Johns Hopkins University)</li>
              <li><a href="https://lizw14.github.io/">Zhuowan Li</a> (affiliated Ph.D. student from Johns Hopkins University)</li>
              <li><a href="https://weichen582.github.io/">Chen Wei</a> (affiliated Ph.D. student from Johns Hopkins University)</li>
              <li><a href="https://scholar.google.com/citations?user=0WFC2w0AAAAJ">Yuanze Lin</a> (visiting graduate student from University of Washington)</li>
              <li><a href="https://cwchenwang.github.io/">Chen Wang</a> (visiting graduate student from Tsinghua University)</li>
              <li>Peiran Xu (visiting undergraduate student from Shanghai Jiao Tong University)</li>
              <li>Junyang Wu (visiting undergraduate student from Shanghai Jiao Tong University)</li>
              <li>Yiqing Wang (visiting undergraduate student from Shanghai Jiao Tong University)</li>
              <li>Zihao Wei (visiting undergraduate student from University of Michigan)</li>
              <li>Shilong Dong (visiting undergraduate student from Fudan University)</li>
              <li>Shaoyuan Xie</a> (visiting undergraduate student from Huazhong University of Science and Technology)</li>
            </ul>
          </td>
        </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                    <heading>Alumni</heading>
                </td>
            </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <ul>
<!--              <li><strong>[<del><font color="red">Call for Papers</font></del>]</strong> I am co-organizing the Workshop on <a href="https://adv-workshop-2020.github.io/">Adversarial Machine Learning in Computer Vision</a> in CVPR 2020. <del>Please submit your paper <a href="https://cmt3.research.microsoft.com/CVPRamlcv2020">here</a>!</del></li>-->
			  <li><a href="https://yingwei.li/">Yingwei Li</a> (affiliated Ph.D. student from Johns Hopkins University; next: Research Scientist at Waymo)</li>
              <li><a href="https://tingxueronghua.github.io/">Yucheng Han</a> (visiting undergraduate student; next: Ph.D. at Nanyang Technological University)</li>
              <li><a href="https://oliverrensu.github.io/">Sucheng Ren</a> (visiting graduate student; next: Ph.D. at UT Austin)</li>
              <li><a href="https://shallowtoil.github.io/">Jinghao Zhou</a> (visiting undergraduate student)</li>
            </ul>
          </td>
        </tr>
        </tbody></table>
        <hr>
        
        
        
<!--         <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                    <heading>Recent Talks</heading>
                </td>
            </tr>
        </tbody></table> -->
        
 <!--        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <ul>
                    <li>Dec 2019: <b>Towards Robust Defense Against Adversarial Examples &amp Beyond</b>, University of Maryland, College Park</li>
                    <li>Dec 2019: <b>Adversarial Examples Improve Image Recognition</b>, Google Brain</li>
                    <li>Sep 2019: <b>Feature Denoising for Improving Adversarial Robustness</b>, VALSE Webinar</li>
                    <li>Summer 2019: <b>Intriguing Adversarial Examples &amp How To Defend Against Them</b>, UC Berkeley, UC San Diego, UC Davis, UC Merced, Stanford University, Gooole Brain</li>
                    <li>May 2019: <b>Towards Transferable Adversarial Attacks & Robust Adversarial Defense</b>, Princeton University</li>
                </ul>
            </tr>
        <hr> -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <h4><u>2022</u></h4>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> 


        <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/wang2022robustcnn.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2206.03452">
                    <papertitle>Can CNNs Be More Robust Than Transformers?</papertitle>
                </a>
                <br>
                <a href="https://zeyuwang.netlify.app/">Zeyu Wang</a>,
                <a href="https://scholar.google.com/citations?user=N1-l4GsAAAAJ">Yutong Bai</a>,
                <a href="https://yuyinzhou.github.io/">Yuyin Zhou</a>,
                <strong>Cihang Xie</strong>
                <br>
                <em>arxiv</em>, 2022
                <br>
                </p>
                <div class="paper" id="wang2022robustcnn">
                    <a href="https://arxiv.org/pdf/2206.03452.pdf">pdf</a> /
                    <a href="https://github.com/UCSC-VLAA/RobustCNN">project page</a> /
                    <a href="data/wang2022robustcnn.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--wang2022robustcnn-->


        <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/bai2022dmae.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2208.12256">
                    <papertitle>Masked Autoencoders Enable Efficient Knowledge Distillers</papertitle>
                </a>
                <br>
                <a href="https://scholar.google.com/citations?user=N1-l4GsAAAAJ">Yutong Bai</a>,
                <a href="https://zeyuwang.netlify.app/">Zeyu Wang</a>,
                <a href="https://scholar.google.com/citations?user=rv-aTqkAAAAJ">Junfei Xiao</a>,
                <a href="https://weichen582.github.io/">Chen Wei</a>,
                <a href="https://csrhddlam.github.io/">Huiyu Wang</a>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>,
                <a href="https://yuyinzhou.github.io/">Yuyin Zhou</a>,
                <strong>Cihang Xie</strong>
                <br>
                <em>arxiv</em>, 2022
                <br>
                </p>
                <div class="paper" id="bai2022dmae">
                    <a href="https://arxiv.org/pdf/2208.12256.pdf">pdf</a> /
                    <a href="https://github.com/UCSC-VLAA/DMAE">project page</a> /
                    <a href="data/bai2022dmae.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--bai2022dmae-->


        <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/li2022bag.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2209.02684">
                    <papertitle>Bag of Tricks for FGSM Adversarial Training</papertitle>
                </a>
                <br>
                <a href="https://zichaoli.github.io/">Zichao Li</a>,
                <a href="https://leolee7.github.io/">Li Liu</a>,
                <a href="https://zeyuwang.netlify.app/">Zeyu Wang</a>,
                <a href="https://yuyinzhou.github.io/">Yuyin Zhou</a>,
                <strong>Cihang Xie</strong>
                <br>
                <em>arxiv</em>, 2022
                <br>
                </p>
                <div class="paper" id="li2022bag">
                    <a href="https://arxiv.org/pdf/2209.02684.pdf">pdf</a> /
                    <a href="https://github.com/UCSC-VLAA/Bag-of-Tricks-for-FGSM-AT">project page</a> /
                    <a href="data/li2022bag.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--li2022bag-->


        <tr>
          <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/nataniel2022counterfactualtest.png' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="">
                    <papertitle>Finding Differences Between Transformers and ConvNets Using Counterfactual Simulation Testing</papertitle>
                </a>
                <br>

                <a href="https://natanielruiz.github.io/">Nataniel Ruiz</a>,
                <strong>Cihang Xie</strong>,
                <a href="https://cs-people.bu.edu/sbargal/">Sarah Adel Bargal</a>,
                <a href="http://ai.bu.edu/ksaenko.html">Kate Saenko</a>,
                <a href="https://www.cs.bu.edu/fac/sclaroff/">Stan Sclaroff</a>
                <br>
                <em>NeurIPS</em>, 2022
                <br>
                </p>
                <div class="paper" id="nataniel2022counterfactualtest">
                    <a href="">pdf (coming soon)</a> /
                    <a href="data/nataniel2022counterfactualtest.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--nataniel2022counterfactualtest-->

        <tr>
          <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/chen2022AAA.png' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2205.12134">
                    <papertitle>Adversarial Attack on Attackers: Post-Process to Mitigate Black-Box Score-Based Query Attacks</papertitle>
                </a>
                <br>

                <a href="https://scholar.google.com.hk/citations?user=X-JmE9UAAAAJ">Sizhe Chen</a>,
                <a href="">Zhehao Huang</a>,
                <a href="https://www.kuleuven.be/wieiswie/en/person/00140240">Qinghua Tao</a>,
                <a href="">Yingwen Wu</a>,
                <strong>Cihang Xie</strong>,
                <a href="https://scholar.google.com/citations?user=DR-gBcEAAAAJ">Xiaolin Huang</a>
                <br>
                <em>NeurIPS</em>, 2022
                <br>
                </p>
                <div class="paper" id="chen2022AAA">
                    <a href="https://arxiv.org/pdf/2205.12134.pdf">pdf</a> /
                    <a href="data/chen2022AAA.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--chen2022AAA-->



        <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/li2022videopretraining.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2205.01721">
                    <papertitle>In Defense of Image Pre-Training for Spatiotemporal Recognition</papertitle>
                </a>
                <br>
                <a href="https://xhl-video.github.io/xianhangli/">Xianhang Li</a>,
                <a href="https://csrhddlam.github.io/">Huiyu Wang</a>,
                <a href="https://weichen582.github.io/">Chen Wei</a>,
                <a href="https://meijieru.com/">Jieru Mei</a>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>,
                <a href="https://yuyinzhou.github.io/">Yuyin Zhou</a>,
                <strong>Cihang Xie</strong>
                <br>
                <em>ECCV</em>, 2022
                <br>
                </p>
                <div class="paper" id="li2022videopretraining">
                    <a href="https://arxiv.org/pdf/2205.01721.pdf">pdf</a> /
                    <a href="https://github.com/UCSC-VLAA/Image-Pretraining-for-Video">project page</a> /
                    <a href="data/li2022videopretraining.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--li2022videopretraining-->

        <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/li2022vip.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="data/li2022vip.pdf">
                    <papertitle>ViP: Unified Certified Detection and Recovery for Patch Attack with Vision Transformers</papertitle>
                </a>
                <br>
                <a href="https://ljb121002.github.io/">Junbo Li</a>,
                <a href="https://www.huan-zhang.com/">Huan Zhang</a>,
                <strong>Cihang Xie</strong>
                <br>
                <em>ECCV</em>, 2022
                <br>
                </p>
                <div class="paper" id="li2022vip">
                    <a href="data/li2022vip.pdf">pdf</a> /
                    <a href="data/li2022vip.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--li2022vip-->
        
        <tr>
          <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/ren2022sdmp.png' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2206.07692">
                    <papertitle>A Simple Data Mixing Prior for Improving Self-Supervised Learning</papertitle>
                </a>
                <br>

                <a href="https://oliverrensu.github.io/">Sucheng Ren</a>,
                <a href="https://csrhddlam.github.io/">Huiyu Wang</a>,
                <a href="https://zhengqigao.github.io/">Zhengqi Gao</a>,
                <a href="http://www.shengfenghe.com/">Shengfeng He</a>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>,
                <a href="https://yuyinzhou.github.io/">Yuyin Zhou</a>,
                <strong>Cihang Xie</strong>
                <br>
                <em>CVPR</em>, 2022
                <br>
                </p>
                <div class="paper" id="ren2022sdmp">
                    <a href="https://arxiv.org/pdf/2206.07692.pdf">pdf</a> /
                    <a href="https://github.com/OliverRensu/SDMP">project page</a> /
                    <a href="data/ren2022sdmp.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--ren2022sdmp-->

        <tr>
          <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/nataniel2021testface.png' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2106.04569">
                    <papertitle>Simulated Adversarial Testing of Face Recognition Models</papertitle>
                </a>
                <br>

                <a href="https://natanielruiz.github.io/">Nataniel Ruiz</a>,
                <a href="https://adamkortylewski.com/">Adam Kortylewski</a>,
                <a href="https://weichaoqiu.com/">Weichao Qiu</a>,
                <strong>Cihang Xie</strong>,
                <a href="https://cs-people.bu.edu/sbargal/">Sarah Adel Bargal</a>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>,
                <a href="https://www.cs.bu.edu/fac/sclaroff/">Stan Sclaroff</a>
                <br>
                <em>CVPR</em>, 2022
                <br>
                </p>
                <div class="paper" id="nataniel2021testface">
                    <a href="https://arxiv.org/pdf/2106.04569.pdf">pdf</a> /
                    <a href="data/nataniel2021testface.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--nataniel2021testface-->

        <tr>
          <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/zhou2022L2B.png' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2202.04291">
                    <papertitle>Learning to Bootstrap for Combating Label Noise</papertitle>
                </a>
                <br>

                <a href="https://yuyinzhou.github.io/">Yuyin Zhou</a>,
                <a href="https://xhl-video.github.io/xianhangli/">Xianhang Li</a>,
                <a href="https://scholar.google.com/citations?user=T3EjsaAAAAAJ&hl=en">Fengze Liu</a>,
                <a href="https://xxchen.site/">Xuxi Chen</a>,
                <a href="https://yulequan.github.io/">Lequan Yu</a>,
                <strong>Cihang Xie</strong>,
                <a href="https://scholar.google.com/citations?user=z1UtMSYAAAAJ&hl=en">Matthew P. Lungren</a>,
                <a href="https://profiles.stanford.edu/lei-xing">Lei Xing</a>
                <br>
                <em>arxiv</em>, 2022
                <br>
                </p>
                <div class="paper" id="zhou2022L2B">
                    <a href="https://arxiv.org/pdf/2202.04291.pdf">pdf</a> /
                    <a href="https://github.com/yuyinzhou/L2B">project page</a> /
                    <a href="data/zhou2022L2B.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--zhou2022L2B-->


        <tr>
          <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/wu2022onepixel.png' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2205.12141">
                    <papertitle>One-Pixel Shortcut: on the Learning Preference of Deep Neural Networks</papertitle>
                </a>
                <br>

                <a href="http://www.pami.sjtu.edu.cn/En/Shutong%20Wu">Shutong Wu</a>,
                <a href="https://scholar.google.com.hk/citations?user=X-JmE9UAAAAJ">Sizhe Chen</a>,
                <strong>Cihang Xie</strong>,
                <a href="https://scholar.google.com/citations?user=DR-gBcEAAAAJ">Xiaolin Huang</a>
                <br>
                <em>arxiv</em>, 2022
                <br>
                </p>
                <div class="paper" id="wu2022onepixel">
                    <a href="https://arxiv.org/pdf/2205.12141.pdf">pdf</a> /
                    <a href="data/wu2022onepixel.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--wu2022onepixel-->


        <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/mei2022fastadvprop.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://openreview.net/forum?id=hcoswsDHNAW">
                    <papertitle>Fast AdvProp</papertitle>
                </a>
                <br>
                <a href="https://meijieru.com/">Jieru Mei</a>,
                <a href="https://tingxueronghua.github.io/">Yucheng Han</a>,
                <a href="https://scholar.google.com/citations?user=N1-l4GsAAAAJ">Yutong Bai</a>,
                <a href="https://scholar.google.com/citations?user=lU3wroMAAAAJ">Yixiao Zhang</a>,
                <a href="https://yingwei.li/">Yingwei Li</a>,
                <a href="https://xhl-video.github.io/xianhangli/">Xianhang Li</a>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>,
                <strong>Cihang Xie</strong>              
                <br>
                <em>ICLR</em>, 2022
                <br>
                </p>
                <div class="paper" id="mei2022fastadvprop">
                    <a href="https://openreview.net/pdf?id=hcoswsDHNAW">pdf</a> /
                    <a href="https://github.com/meijieru/fast_advprop">project page</a> /
                    <a href="data/mei2022fastadvprop.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--mei2022fastadvprop-->

        <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/zhou2021ibot.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2111.07832">
                    <papertitle>iBOT: Image BERT Pre-Training with Online Tokenizer</papertitle>
                </a>
                <br>
                <a href="https://shallowtoil.github.io/">Jinghao Zhou</a>,
                <a href="https://weichen582.github.io/">Chen Wei</a>,
                <a href="https://csrhddlam.github.io/">Huiyu Wang</a>,
                <a href="http://wei-shen.weebly.com/">Wei Shen</a>,
                <strong>Cihang Xie</strong>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>,
                <a href="http://www.taokong.org/">Tao Kong</a>
                <br>
                <em>ICLR</em>, 2022
                <br>
                </p>
                <div class="paper" id="zhou2021ibot">
                    <a href="https://arxiv.org/pdf/2111.07832.pdf">pdf</a> /
                    <a href="https://github.com/bytedance/ibot">project page</a> /
                    <a href="https://medium.com/syncedreview/meet-ibot-a-masked-image-modelling-framework-that-enables-bert-like-pretraining-for-vision-da01002115e7">press</a> /
                    <a href="data/zhou2021ibot.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--zhou2021ibot-->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <h4><u>2021</u></h4>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>        


        <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/bai2020vitsVScnns.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2111.05464">
                    <papertitle>Are Transformers More Robust Than CNNs?</papertitle>
                </a>
                <br>
                <a href="https://scholar.google.com/citations?user=N1-l4GsAAAAJ">Yutong Bai</a>,
                <a href="https://meijieru.com/">Jieru Mei</a>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>,
                <strong>Cihang Xie</strong>
                <br>
                <em>NeurIPS</em>, 2021
                <br>
                <font color="orange"><strong>The first benchmark that fairly compares Transformers with CNNs on robustness evaluations</strong></font>
                <br>
                </p>
                <div class="paper" id="bai2020vitsVScnns">
                    <a href="https://arxiv.org/pdf/2111.05464.pdf">pdf</a> /
                    <a href="https://github.com/ytongbai/ViTs-vs-CNNs">project page</a> /
                    <a href="data/bai2020vitsVScnns.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--bai2020vitsVScnns-->

        <tr>
            <td style="padding:20px;width:35%;vertical-align:top">
                <img src='images/li2021calico.png' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2110.00519">
                    <papertitle>Calibrating Concepts and Operations: Towards Symbolic Reasoning on Real Images</papertitle>
                </a>
                <br>

                <a href="https://lizw14.github.io/">Zhuowan Li</a>,
                <a href="https://esteng.github.io/">Elias Stengel-Eskin</a>,
                <a href="https://scholar.google.com/citations?user=lU3wroMAAAAJ">Yixiao Zhang</a>,
                <strong>Cihang Xie</strong>,
                <a href="https://research.adobe.com/person/quan-hung-tran/">Quan Tran</a>,
                <a href="https://www.cs.jhu.edu/~vandurme/">Benjamin Van Durme</a>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>
                <br>
                <em>ICCV</em>, 2021
                <br>
                </p>
                <div class="paper" id="li2021calico">
                    <a href="https://arxiv.org/pdf/2110.00519.pdf">pdf</a> /
                    <a href="https://github.com/Lizw14/CaliCO">project page</a> /
                    <a href="data/li2021calico.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--li2021calico-->

        <tr>
            <td style="padding:20px;width:35%;vertical-align:top">
                <img src='images/chen2021detadvprop.png' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2103.13886">
                    <papertitle>Robust and Accurate Object Detection via Adversarial Learning</papertitle>
                </a>
                <br>
                <a href="http://web.cs.ucla.edu/~xiangning/">Xiangning Chen</a>,
                <strong>Cihang Xie</strong>,
                <a href="https://scholar.google.com/citations?user=6POeyBoAAAAJ&hl=en">Mingxing Tan</a>,
                <a href="https://research.google/people/105588/">Li Zhang</a>,
                <a href="http://web.cs.ucla.edu/~chohsieh/">Cho-Jui Hsieh</a>,
                <a href="http://boqinggong.info/">Boqing Gong</a>
                <br>
                <em>CVPR</em>, 2021
                <br>
                </p>
                <div class="paper" id="chen2021detadvprop">
                    <a href="https://arxiv.org/pdf/2103.13886.pdf">pdf</a> /
                    <a href="https://github.com/google/automl/blob/master/efficientdet/Det-AdvProp.md">project page</a> /
                    <a href="data/chen2021detadvprop.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--chen2021detadvprop-->
        
        <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:35%;vertical-align:top">
                <img src='images/li2020shapetexture.jpg' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2010.05981">
                    <papertitle>Shape-Texture Debiased Neural Network Training</papertitle>
                </a>
                <br>
                <a href="https://yingwei.li/">Yingwei Li</a>,
                <a href="https://yucornetto.github.io/">Qihang Yu</a>,
                <a href="https://scholar.google.com/citations?user=6POeyBoAAAAJ&hl=en">Mingxing Tan</a>,
                <a href="https://meijieru.com/">Jieru Mei</a>,
                <a href="http://pengtang.xyz/">Peng Tang</a>,
                <a href="http://wei-shen.weebly.com/">Wei Shen</a>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>,
                <strong>Cihang Xie</strong>
                <br>
                <em>ICLR</em>, 2021
                <br>
                </p>
                <div class="paper" id="li2020shapetexture">
                    <a href="https://arxiv.org/pdf/2010.05981.pdf">pdf</a> /
                    <a href="https://github.com/LiYingwei/ShapeTextureDebiasedTraining">project page</a> /
                    <a href="data/li2020shapetexture.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--li2020shapetexture-->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <h4><u>2020</u></h4>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> 

        <tr>
            <td style="padding:20px;width:35%;vertical-align:top">
                <img src='images/JHU.jpg' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://jscholarship.library.jhu.edu/handle/1774.2/63790">
                    <papertitle>Towards Robust Representation Learning and Beyond</papertitle>
                </a>
                <br>
                <strong>Cihang Xie</strong>
                <br>
                <em>PhD Dissertation</em>
                <br>
                </p>
                <div class="paper" id="xie2020phd">
                    <a href="data/phd_thesis.pdf">pdf</a> /
                    <a href="data/xie2020phd.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--xie2020phd-->

        <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/xie2020sat.jpg' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2006.14536">
                    <papertitle>Smooth Adversarial Training</papertitle>
                </a>
                <br>
                <strong>Cihang Xie</strong>,
                <a href="https://scholar.google.com/citations?user=6POeyBoAAAAJ&hl=en">Mingxing Tan</a>,
                <a href="http://boqinggong.info/">Boqing Gong</a>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>,
                <a href="https://cs.stanford.edu/~quocle/">Quoc Le</a>
                <br>
                <em>Tech report</em>, arXiv
                <br>
                <font color="orange"><strong>State-of-the-art method for defending against adversarial attacks on ImageNet</strong></font>
                <br>
                </p>
                <div class="paper" id="xie2020sat">
                    <a href="https://arxiv.org/pdf/2006.14536.pdf">pdf</a> /
                    <a href="https://github.com/cihangxie/SmoothAdversarialTraining">project page</a> /
                    <a href="data/xie2020sat.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--xie2020sat-->

        <tr>
            <td style="padding:20px;width:35%;vertical-align:top">
                <img src='images/xu2020bnet.jpg' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2011.14150">
                    <papertitle>Batch Normalization with Enhanced Linear Transformation</papertitle>
                </a>
                <br>

                <a href="https://yuhuixu1993.github.io/">Yuhui Xu</a>,
                <a href="http://lingxixie.com/Home.html">Lingxi Xie</a>,
                <strong>Cihang Xie</strong>,
                <a href="https://meijieru.com/">Jieru Mei</a>,
                <a href="https://www.cs.jhu.edu/~syqiao/">Siyuan Qiao</a>,
                <a href="http://wei-shen.weebly.com/">Wei Shen</a>,
                <a href="http://min.sjtu.edu.cn/xhk.htm">Hongkai Xiong</a>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>                
                <br>
                <em>Tech report</em>, arXiv
                <br>
                </p>
                <div class="paper" id="xu2020bnet">
                    <a href="https://arxiv.org/pdf/2011.14150.pdf">pdf</a> /
                    <a href="https://github.com/yuhuixu1993/BNET">project page</a> /
                    <a href="data/xu2020bnet.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--xu2020bnet-->

        <tr>
            <td style="padding:20px;width:35%;vertical-align:top">
                <img src='images/yang2020patchattack.png' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2004.05682">
                    <papertitle>PatchAttack: A Black-box Texture-based Attack with Reinforcement Learning</papertitle>
                </a>
                <br>
                <a href="https://chenglin-yang.github.io/">Chenglin Yang</a>,
                <a href="https://adamkortylewski.com/">Adam Kortylewski</a>,
                <strong>Cihang Xie</strong>,
                <a href="https://www.yinzhicao.org/">Yinzhi Cao</a>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>
                <br>
                <em>ECCV</em>, 2020
                <br>
                </p>
                <div class="paper" id="yang2020patchattack">
                    <a href="https://arxiv.org/pdf/2004.05682.pdf">pdf</a> /
                    <a href="https://github.com/Chenglin-Yang/PatchAttack">project page</a> /
                    <a href="data/yang2020patchattack.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--yang2020patchattack-->
        
        <tr>
            <td style="padding:20px;width:35%;vertical-align:top">
                <img src='images/li2019regional.png' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/1904.00979">
                    <papertitle>Regional Homogeneity: Towards Learning Transferable Universal Adversarial Perturbations Against Defenses</papertitle>
                </a>
                <br>
                <a href="https://yingwei.li/">Yingwei Li</a>,
                <a href="http://songbai.site/">Song Bai</a>,
                <strong>Cihang Xie</strong>,
                <a href="https://www.researchgate.net/profile/Zhenyu_Liao4">Zhenyu Liao</a>,
                <a href="http://users.eecs.northwestern.edu/~xsh835/">Xiaohui Shen</a>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>
                <br>
                <em>ECCV</em>, 2020
                <br>
                </p>
                <div class="paper" id="li2019regional">
                    <a href="https://arxiv.org/pdf/1904.00979.pdf">pdf</a> /
                    <a href="https://github.com/LiYingwei/Regional-Homogeneity">project page</a> /
                    <a href="data/li2019regional.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--li2019regional-->
            
        <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/xie2020advprop.png' width="250"></div>
            </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/1911.09665">
                    <papertitle>Adversarial Examples Improve Image Recognition</papertitle>
                </a>
                <br>
                <strong>Cihang Xie</strong>,
                <a href="https://scholar.google.com/citations?user=6POeyBoAAAAJ&hl=en">Mingxing Tan</a>,
                <a href="http://boqinggong.info/">Boqing Gong</a>,
                <a href="http://wangjiangb.github.io/">Jiang Wang</a>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>,
                <a href="https://cs.stanford.edu/~quocle/">Quoc Le</a>
                <br>
                <em>CVPR</em>, 2020
                <br>
                <font color="orange"><strong>State-of-the-art ImageNet classifier without extra training data</strong></font>
                <br>
                </p>
                <div class="paper" id="xie2020advprop">
                    <a href="https://arxiv.org/pdf/1911.09665.pdf">pdf</a> /
                    <a href="https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet">project page</a> /
                    <a href="https://github.com/tingxueronghua/pytorch-classification-advprop">pytorch reimplementation</a> /
                    <a href="data/AdvProp_Talk.pdf">slides</a> /
                    <a href="https://medium.com/syncedreview/google-johns-hopkins-university-can-adversarial-examples-improve-image-recognition-bcb7254e2d8">press</a> /
                    <a href="https://www.youtube.com/watch?v=KTCztkNJm50">video</a> /
                    <a href="data/xie2020advprop.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--xie2020advprop-->
        
        <tr>
            <td style="padding:20px;width:35%;vertical-align:top">
                <img src='images/li2020nas.png' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2004.01961">
                    <papertitle>Neural Architecture Search for Lightweight Non-Local Networks</papertitle>
                </a>
                <br>
                <a href="https://yingwei.li/">Yingwei Li</a>,
                <a href="https://scholar.google.com/citations?user=OEZ816YAAAAJ">Xiaojie Jin</a>,
                <a href="https://meijieru.com/">Jieru Mei</a>,
                <a href="https://scholar.google.com/citations?user=-jNTDU0AAAAJ&hl">Xiaochen Lian</a>,
                <a href="https://sites.google.com/site/linjieyang89/">Linjie Yang</a>,
                <strong>Cihang Xie</strong>,
                <a href="https://yucornetto.github.io/">Qihang Yu</a>,
                <a href="https://yuyinzhou.github.io/">Yuyin Zhou</a>,
                <a href="http://songbai.site/">Song Bai</a>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>
                <br>
                <em>CVPR</em>, 2020
                <br>
                </p>
                <div class="paper" id="li2020nas">
                    <a href="https://arxiv.org/pdf/2004.01961.pdf">pdf</a> /
                    <a href="https://github.com/LiYingwei/AutoNL">project page</a> /
                    <a href="data/li2020nas.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--li2020nas-->
  
        <tr>
            <td style="padding:20px;width:35%;vertical-align:top">
                <img src='images/huang2019upc.gif' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/1909.04326">
                    <papertitle>Universal Physical Camouflage Attacks on Object Detectors</papertitle>
                </a>
                <br>
                <a href="">Lifeng Huang</a>,
                <a href="http://sdcs.sysu.edu.cn/content/2537">Chengying Gao</a>,
                <a href="https://yuyinzhou.github.io/">Yuyin Zhou</a>,
                <strong>Cihang Xie</strong>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>,
                <a href="https://changqingzou.weebly.com/">Changqing Zou</a>,
                <a href="http://sdcs.sysu.edu.cn/node/2495">Ning Liu</a>
                <br>
                <em>CVPR</em>, 2020
                <br>
                </p>
                <div class="paper" id="huang2019upc">
                    <a href="https://arxiv.org/pdf/1909.04326.pdf">pdf</a> /
                    <a href="https://mesunhlf.github.io/index_physical.html">project page</a> /
                    <a href="data/huang2019upc.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--huang2019upc-->
                        
        <tr>
          <td style="padding:20px;width:35%;vertical-align:middle">
              <img src='images/xie2020intriguing.png' width="250"></div>
          </td>
          <td width="75%" valign="middle">
            <p>
              <a href="https://arxiv.org/abs/1906.03787">
                <papertitle>Intriguing Properties of Adversarial Training at Scale</papertitle>
              </a>
              <br>
              <strong>Cihang Xie</strong>,
              <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>
              <br>
              <em>ICLR</em>, 2020
              <br>
            </p>
            <div class="paper" id="xie2020intriguing">
              <a href="https://arxiv.org/pdf/1906.03787.pdf">pdf</a> /
              <a href="data/xie2020intriguing.bib">bibtex</a>
            </div>
            <br>
          </td>
        </tr> <!--xie2020intriguing-->
        
        <tr>
            <td style="padding:20px;width:35%;vertical-align:top">
                <img src='images/li2020learning.png' width="250"></div>
            </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/1812.03413">
                    <papertitle>Learning Transferable Adversarial Examples via Ghost Networks</papertitle>
                </a>
                <br>
                <a href="https://yingwei.li/">Yingwei Li</a>,
                <a href="http://songbai.site/">Song Bai</a>,
                <a href="https://yuyinzhou.github.io/">Yuyin Zhou</a>,
                <strong>Cihang Xie</strong>,
                <a href="https://zhishuai.xyz/">Zhishuai Zhang</a>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>
                <br>
                <em>AAAI</em>, 2020
                <br>
                </p>
                <div class="paper" id="li2020learning">
                    <a href="https://arxiv.org/pdf/1812.03413.pdf">pdf</a> /
                    <a href="https://github.com/LiYingwei/ghost-network">project page</a> /
                    <a href="data/li2020learning.bib">bibtex</a>
                </div>
                <br>
              </td>
          </tr> <!--li2020learning-->


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <h4><u>2019</u></h4>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> 

        <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/xie2019denoising.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/1812.03411">
                    <papertitle>Feature Denoising for Improving Adversarial Robustness</papertitle>
                </a>
                <br>
                <strong>Cihang Xie</strong>,
                <a href="http://ppwwyyxx.com/">Yuxin Wu</a>,
                <a href="https://lvdmaaten.github.io/">Laurens van der Maaten</a>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>,
                <a href="http://kaiminghe.com/">Kaiming He</a>
                <br>
                <em>CVPR</em>, 2019
                <br>
                <font color="orange"><strong>The first ImageNet classifier that can successfully defend against strong white-box adversarial attacks</strong></font>
                <br>
                </p>
                <div class="paper" id="xie2019denoising">
                    <a href="https://arxiv.org/pdf/1812.03411.pdf">pdf</a> /
                    <a href="https://github.com/facebookresearch/ImageNet-Adversarial-Training">project page</a> /
                    <a href="data/Feature_Denoise.pdf">slides</a> /
                    <a href="https://ai.facebook.com/blog/feature-denoising/">blog</a> /
                    <a href="data/xie2019denoising.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--xie2019denoising-->
        
        <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/xie2019DiverseInput.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/1803.06978">
                    <papertitle>Improving Transferability of Adversarial Examples with Input Diversity</papertitle>
                </a>
                <br>
                <strong>Cihang Xie</strong>,
                <a href="https://zhishuai.xyz/">Zhishuai Zhang</a>,
                <a href="https://yuyinzhou.github.io/">Yuyin Zhou</a>,
                <a href="http://songbai.site/">Song Bai</a>,
                <a href="http://www.jianyuwang.org/">Jianyu Wang</a>,
                <a href="http://web.cs.ucla.edu/~zhou.ren/">Zhou Ren</a>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>
                <br>
                <em>CVPR</em>, 2019
                <br>
                </p>
                <div class="paper" id="xie2019DiverseInput">
                    <a href="https://arxiv.org/pdf/1803.06978.pdf">pdf</a> /
                    <a href="https://github.com/cihangxie/DI-2-FGSM">project page</a> /
                    <a href="data/xie2019DiverseInput.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--xie2019DiverseInput-->


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <h4><u>2018</u></h4>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> 
        
        <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/xie2018randomization.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/1711.01991">
                    <papertitle>Mitigating Adversarial Effects Through Randomization</papertitle>
                </a>
                <br>
                <strong>Cihang Xie</strong>,
                <a href="http://www.jianyuwang.org/">Jianyu Wang</a>,
                <a href="https://zhishuai.xyz/">Zhishuai Zhang</a>,
                <a href="http://web.cs.ucla.edu/~zhou.ren/">Zhou Ren</a>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>
                <br>
                <em>ICLR</em>, 2018
                <br>
                <font color="orange"><strong>The runner-up solution in the adversarial defense track of NIPS 2017 Adversarial Attacks and Defenses Competition</strong></font>
                <br>
                </p>
                <div class="paper" id="xie2018randomization">
                    <a href="https://arxiv.org/pdf/1711.01991.pdf">pdf</a> /
                    <a href="https://github.com/cihangxie/NIPS2017_adv_challenge_defense">project page</a> /
                    <a href="https://arxiv.org/pdf/1804.00097.pdf">competition book</a> /
                    <a href="data/NIPS_ADV.pdf">slides</a> /
                    <a href="data/xie2018randomization.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--xie2018randomization-->
        
        <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/zhang2018des.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/1712.00433">
                    <papertitle>Single-Shot Object Detection with Enriched Semantics</papertitle>
                </a>
                <br>
                <a href="https://zhishuai.xyz/">Zhishuai Zhang</a>,
                <a href="https://www.cs.jhu.edu/~syqiao/">Siyuan Qiao</a>,
                <strong>Cihang Xie</strong>,
                <a href="http://wei-shen.weebly.com/">Wei Shen</a>,
                <a href="https://vectorinstitute.ai/team/bo-wang/">Bo Wang</a>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>
                <br>
                <em>CVPR</em>, 2018
                <br>
                </p>
                <div class="paper" id="zhang2018des">
                    <a href="https://arxiv.org/pdf/1712.00433.pdf">pdf</a> /
                    <a href="https://github.com/bairdzhang/des">project page</a> /
                    <a href="data/zhang2018des.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--zhang2018des-->
        
        <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/zhang2018deepvoting.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/1709.04577">
                    <papertitle>DeepVoting: A Robust and Explainable Deep Network for Semantic Part Detection under Partial Occlusion</papertitle>
                </a>
                <br>
                <a href="https://zhishuai.xyz/">Zhishuai Zhang</a>,
                <strong>Cihang Xie</strong>,
                <a href="http://www.jianyuwang.org/">Jianyu Wang</a>,
                <a href="http://lingxixie.com/Home.html">Lingxi Xie</a>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>
                <br>
                <em>CVPR</em>, 2018
                <br>
                </p>
                <div class="paper" id="zhang2018deepvoting">
                    <a href="https://arxiv.org/pdf/1709.04577.pdf">pdf</a> /
                    <a href="data/zhang2018deepvoting.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--zhang2018deepvoting-->
        
        <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
            <td style="padding:20px;width:35%;vertical-align:middle">
                <div class="one">
                    <div class="two" id='wang2018vcsp'><img src='images/penguin.png' width="250"></div>
                    <img src='images/monkey.png' width="250">
                    </div>
                <script type="text/javascript">
                    function portrait_start() {
                        document.getElementById('wang2018vcsp').style.opacity = "1";
                    }
                
                function portrait_stop() {
                    document.getElementById('wang2018vcsp').style.opacity = "0";
                }
                portrait_stop()
                </script>
            </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/1711.04451">
                    <papertitle>Visual Concepts and Compositional Voting</papertitle>
                </a>
                <br>
                <a href="http://www.jianyuwang.org/">Jianyu Wang</a>,
                <a href="https://zhishuai.xyz/">Zhishuai Zhang</a>,
                <strong>Cihang Xie</strong>,
                <a href="https://yuyinzhou.github.io/">Yuyin Zhou</a>,
                <a href="http://vittalp.github.io/">Vittal Premachandran</a>,
                <a href="https://scholar.google.com/citations?user=4e5ajP4AAAAJ&hl=en">Jun Zhu</a>,
                <a href="http://lingxixie.com/Home.html">Lingxi Xie</a>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>
                <br>
                <em>Annals of Mathematical Sciences and Applications</em>, 2018
                <br>
                </p>
                <div class="paper" id="wang2018vcsp">
                    <a href="https://arxiv.org/pdf/1711.04451.pdf">pdf</a> /
                    <a href="data/wang2018vcsp.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--wang2018vcsp-->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <h4><u>2017</u></h4>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> 
        
        <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/xie2017dag.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/1703.08603">
                    <papertitle>Adversarial Examples for Semantic Segmentation and Object Detection</papertitle>
                </a>
                <br>
                <strong>Cihang Xie</strong>,
                <a href="http://www.jianyuwang.org/">Jianyu Wang</a>,
                <a href="https://zhishuai.xyz/">Zhishuai Zhang</a>,
                <a href="https://yuyinzhou.github.io/">Yuyin Zhou</a>,
                <a href="http://lingxixie.com/Home.html">Lingxi Xie</a>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>
                <br>
                <em>ICCV</em>, 2017
                <br>
                </p>
                <div class="paper" id="xie2017dag">
                    <a href="https://arxiv.org/pdf/1703.08603.pdf">pdf</a> /
                    <a href="https://github.com/cihangxie/DAG">project page</a> /
                    <a href="data/DAG.pdf">slides</a> /
                    <a href="data/xie2017dag.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--xie2017dag-->
        
        <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/wang2017voting.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/1707.07819">
                    <papertitle>Detecting Semantic Parts on Partially Occluded Objects</papertitle>
                </a>
                <br>
                <a href="http://www.jianyuwang.org/">Jianyu Wang</a>,
                <strong>Cihang Xie</strong>,
                <a href="https://zhishuai.xyz/">Zhishuai Zhang</a>,
                <a href="https://scholar.google.com/citations?user=4e5ajP4AAAAJ&hl=en">Jun Zhu</a>,
                <a href="http://lingxixie.com/Home.html">Lingxi Xie</a>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>
                <br>
                <em>BMVC</em>, 2017
                <br>
                </p>
                <div class="paper" id="wang2017voting">
                    <a href="https://arxiv.org/pdf/1707.07819.pdf">pdf</a> /
                    <a href="data/wang2017voting.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--wang2017voting-->
        
        </tbody></table>
        <hr>
        
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                    <heading>Open Sources</heading>
                </td>
            </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        
        <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/cleverhans.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://github.com/tensorflow/cleverhans">
                    <papertitle>CleverHans Adversarial Examples Library</papertitle>
                </a>
                <br>
                </p>
                <div class="paper" id="cleverhans">
                    <a href="https://github.com/tensorflow/cleverhans">project page</a> /
                    <a href="https://arxiv.org/pdf/1610.00768.pdf">tech report</a>
                </div>
            </td>
        </tr> <!--cleverhans-->
        
        <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/faceattack.jpg' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://github.com/ppwwyyxx/Adversarial-Face-Attack">
                    <papertitle>Adversarial Attacks on Face Recognition</papertitle>
                </a>
                <br>
                </p>
                <div class="paper" id="cleverhans">
                    <a href="https://github.com/ppwwyyxx/Adversarial-Face-Attack">project page</a> /
                    <a href="https://www.jiqizhixin.com/articles/2018-10-26-11">press (chinese)</a>
                </div>
            </td>
        </tr> <!--face attack-->
        
        </tbody></table>
        <hr>


<!--        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>-->
<!--          <tr>-->
<!--            <td style="padding:20px;width:100%;vertical-align:middle">-->
<!--              <heading>Service</heading>-->
<!--            </td>-->
<!--          </tr>-->
<!--        </tbody></table>-->
<!--        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>-->
<!--          <tr>-->
<!--            <td style="padding:20px;width:25%;vertical-align:middle"><img src="old-images/cvf.jpg"></td>-->
<!--            <td width="75%" valign="center">-->
<!--              <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>-->
<!--              <br>-->
<!--              <br>-->
<!--              <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>-->
<!--            </td>-->
<!--          </tr>-->
<!--        </tbody></table>-->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td width=50% align="center">
              <a href="https://clustrmaps.com/site/1b2nv"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=_s2_OduoO888ugvX-pYCsciaAlSir2c84N9WH4N3AmQ&cl=ffffff" /></a>
            </td>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;">Stolen from <a href="https://jonbarron.info/">Jon Barron</a></p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>

<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>

<!-- Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-131560165-1', 'auto');
  ga('send', 'pageview');
</script>
<!-- End Google Analytics -->

</body>

</html>
